# -*- coding: utf-8 -*-
"""Blockflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dyFxpvd-gaPhI0KUfZEkEPH0xoR5cE2k
"""

# Run this cell first
!pip install torch torchvision matplotlib scikit-learn cryptography tqdm pydantic numpy pandas web3

# BlockFlow:Colab Implementation
# =======================================
import torch
import torch.nn as nn
import numpy as np
from tqdm.notebook import tqdm
from typing import List, Dict, Any, Optional, Tuple
import time
import matplotlib.pyplot as plt
import hashlib
import pickle
import json
import os
import random
from dataclasses import dataclass
import threading
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric import padding, rsa
from cryptography.hazmat.backends import default_backend
from pydantic import BaseModel

# ==================== CONFIG ====================
class BlockFlowConfig(BaseModel):
    num_clients: int = 100
    num_shards: int = 5
    local_epochs: int = 5
    learning_rate: float = 0.01
    batch_size: int = 32
    model_architecture: str = "cnn"
    model_params: Dict[str, Any] = {"hidden_dim": 128, "num_classes": 10}
    mcl_block_time: float = 0.2
    atl_block_time: float = 12.0
    confirmation_delay: int = 3
    initial_reputation: float = 0.5
    reputation_penalty: float = 0.1
    reputation_reward: float = 0.01
    min_reputation: float = 0.1
    max_reputation: float = 1.0
    total_reward_per_round: float = 1000.0
    shapley_samples: int = 50
    participation_cost: float = 10.0
    use_zksnarks: bool = True
    use_merkle: bool = True
    max_updates_per_second: int = 1200
    ipfs_timeout: int = 30

# ==================== CRYPTO ====================
class MerkleNode:
    def __init__(self, hash_val: bytes, left=None, right=None, data=None):
        self.hash = hash_val
        self.left = left
        self.right = right
        self.data = data

class MerkleTree:
    def __init__(self, data: bytes, chunk_size: int = 1024):
        self.chunk_size = chunk_size
        self.leaves = self._create_leaves(data)
        self.root = self._build_tree(self.leaves)

    def _create_leaves(self, data: bytes) -> List[MerkleNode]:
        leaves = []
        for i in range(0, len(data), self.chunk_size):
            chunk = data[i:i + self.chunk_size]
            leaf_hash = hashlib.sha256(b"\x00" + chunk).digest()
            leaves.append(MerkleNode(leaf_hash, data=chunk))
        return leaves

    def _build_tree(self, nodes: List[MerkleNode]) -> Optional[MerkleNode]:
        if not nodes:
            return None
        while len(nodes) > 1:
            next_level = []
            for i in range(0, len(nodes), 2):
                left = nodes[i]
                right = nodes[i + 1] if i + 1 < len(nodes) else nodes[i]
                combined_hash = hashlib.sha256(b"\x01" + left.hash + right.hash).digest()
                parent = MerkleNode(combined_hash, left, right)
                next_level.append(parent)
            nodes = next_level
        return nodes[0] if nodes else None

    def get_root(self) -> bytes:
        return self.root.hash if self.root else b""

class ZKProver:
    def __init__(self):
        self.setup = self._generate_setup()

    def _generate_setup(self) -> Dict[str, Any]:
        return {
            "proving_key": hashlib.sha256(b"proving_key").digest(),
            "verification_key": hashlib.sha256(b"verification_key").digest(),
        }

    def generate_proof(self, model_update: Dict, merkle_root: bytes) -> bytes:
        proof_data = {"model_hash": self._hash_model_update(model_update), "merkle_root": merkle_root.hex(), "timestamp": time.time()}
        return hashlib.sha256(json.dumps(proof_data, sort_keys=True).encode()).digest()

    def _hash_model_update(self, update: Dict) -> str:
        update_str = ""
        for name, param in sorted(update.items()):
            update_str += f"{name}:{param.abs().sum().item():.6f}"
        return hashlib.sha256(update_str.encode()).hexdigest()

class ZKVerifier:
    def __init__(self, verification_key: bytes):
        self.verification_key = verification_key

    def verify_proof(self, proof: bytes, merkle_root: bytes) -> bool:
        return len(proof) > 0 and not proof.hex().startswith("deadbeef")

# ==================== STORAGE ====================
class IPFSStorage:
    def __init__(self, storage_dir: str = "./ipfs_storage"):
        self.storage_dir = storage_dir
        os.makedirs(storage_dir, exist_ok=True)
        self.cache: Dict[str, Any] = {}

    def store_model(self, model: Dict, pin: bool = True) -> str:
        model_bytes = pickle.dumps(model)
        content_hash = hashlib.sha256(model_bytes).hexdigest()
        ipfs_hash = f"Qm{content_hash[:44]}"
        filepath = os.path.join(self.storage_dir, ipfs_hash)
        with open(filepath, 'wb') as f:
            f.write(model_bytes)
        self.cache[ipfs_hash] = model
        return ipfs_hash

    def retrieve_model(self, ipfs_hash: str) -> Dict:
        if ipfs_hash in self.cache:
            return self.cache[ipfs_hash]
        filepath = os.path.join(self.storage_dir, ipfs_hash)
        if os.path.exists(filepath):
            with open(filepath, 'rb') as f:
                model = pickle.load(f)
                self.cache[ipfs_hash] = model
                return model
        return {}

# ==================== SHARDING ====================
class ShardManager:
    def __init__(self, num_shards: int, config):
        self.num_shards = num_shards
        self.config = config
        self.shard_assignments: Dict[int, int] = {}
        self.shard_aggregators: Dict[int, List[int]] = {}
        self.beacon_chain = []

    def assign_client_to_shard(self, client_id: int, geo_location: str, reputation: float) -> int:
        geo_hash = int(hashlib.sha256(geo_location.encode()).hexdigest(), 16)
        rep_score = int(reputation * 1000)
        shard_id = ((client_id * self.num_shards) // self.config.num_clients + geo_hash + rep_score) % self.num_shards
        self.shard_assignments[client_id] = shard_id
        if shard_id not in self.shard_aggregators:
            self.shard_aggregators[shard_id] = []
        return shard_id

    def get_shard_members(self, shard_id: int) -> List[int]:
        return [cid for cid, sid in self.shard_assignments.items() if sid == shard_id]

    def rebalance_shards(self, client_reputations: Dict[int, float]):
        shard_loads = {i: len(self.get_shard_members(i)) for i in range(self.num_shards)}
        avg_load = sum(shard_loads.values()) / self.num_shards
        overloaded = [sid for sid, load in shard_loads.items() if load > avg_load * 1.2]
        underloaded = [sid for sid, load in shard_loads.items() if load < avg_load * 0.8]
        if overloaded and underloaded:
            for src_shard in overloaded:
                members = self.get_shard_members(src_shard)
                if members:
                    best_client = max(members, key=lambda cid: client_reputations.get(cid, 0))
                    dest_shard = underloaded[0]
                    self.shard_assignments[best_client] = dest_shard
                    print(f"üîÑ Rebalanced client {best_client} from shard {src_shard} to {dest_shard}")
                    break

# ==================== CORE: CLIENT ====================
@dataclass
class ModelUpdate:
    client_id: int
    merkle_root: bytes
    zk_proof: bytes
    signature: bytes
    timestamp: float
    reputation: float = 0.5

class FLClient:
    def __init__(self, client_id: int, data_loader, model: nn.Module, config, private_key: Optional[bytes] = None):
        self.client_id = client_id
        self.data_loader = data_loader
        self.model = model
        self.config = config
        self.private_key = private_key or self._generate_key()
        self.public_key = self._derive_public_key()
        self.reputation = config.initial_reputation
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)

    def _generate_key(self) -> bytes:
        key = rsa.generate_private_key(public_exponent=65537, key_size=2048, backend=default_backend())
        return key.private_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PrivateFormat.PKCS8,
            encryption_algorithm=serialization.NoEncryption()
        )

    def _derive_public_key(self) -> bytes:
        return hashlib.sha256(self.private_key).digest()

    def local_training(self, global_weights: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        self.model.load_state_dict(global_weights)
        self.model.train()
        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.learning_rate)
        criterion = nn.CrossEntropyLoss()

        for epoch in range(self.config.local_epochs):
            for batch_idx, (data, target) in enumerate(self.data_loader):
                data, target = data.to(self.device), target.to(self.device)
                optimizer.zero_grad()
                output = self.model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()

        updates = {}
        for name, param in self.model.named_parameters():
            updates[name] = param.data.cpu() - global_weights[name].cpu()
        return updates

    def generate_commitment(self, updates: Dict[str, torch.Tensor]) -> Tuple[bytes, bytes, bytes]:
        update_vector = torch.cat([u.flatten() for u in updates.values()])
        update_bytes = update_vector.numpy().tobytes()
        merkle_tree = MerkleTree(update_bytes)
        merkle_root = merkle_tree.get_root()

        if self.config.use_zksnarks:
            zk_prover = ZKProver()
            zk_proof = zk_prover.generate_proof(updates, merkle_root)
        else:
            zk_proof = b"zk_proof_placeholder"

        signature = self._sign(merkle_root + zk_proof)
        return merkle_root, zk_proof, signature

    def _sign(self, data: bytes) -> bytes:
        private_key = serialization.load_pem_private_key(
            self.private_key, password=None, backend=default_backend()
        )
        return private_key.sign(
            data,
            padding.PSS(mgf=padding.MGF1(hashes.SHA256()), salt_length=padding.PSS.MAX_LENGTH),
            hashes.SHA256()
        )

    def verify_signature(self, data: bytes, signature: bytes) -> bool:
        try:
            public_key = serialization.load_pem_public_key(self.public_key)
            public_key.verify(
                signature,
                data,
                padding.PSS(mgf=padding.MGF1(hashes.SHA256()), salt_length=padding.PSS.MAX_LENGTH),
                hashes.SHA256()
            )
            return True
        except:
            return False

# ==================== CORE: AGGREGATOR ====================
class Aggregator:
    def __init__(self, aggregator_id: int, config):
        self.aggregator_id = aggregator_id
        self.config = config
        self.storage = IPFSStorage()
        self.shard_members = []

    def aggregate_models(self, updates: List[Dict[str, torch.Tensor]], reputations: List[float], dataset_sizes: List[int]) -> Tuple[Dict, str]:
        if not updates:
            raise ValueError("No updates to aggregate")
        total_weight = sum(rep * size for rep, size in zip(reputations, dataset_sizes))
        weights = [rep * size / total_weight for rep, size in zip(reputations, dataset_sizes)]
        aggregated_update = {name: torch.zeros_like(param) for name, param in updates[0].items()}

        for update, weight in zip(updates, weights):
            for name, param in update.items():
                aggregated_update[name] += weight * param

        model_hash = self.storage.store_model(aggregated_update)
        return aggregated_update, model_hash

# ==================== CORE: MCL CONTRACT ====================
class MCLContract:
    def __init__(self, config):
        self.config = config
        self.updates: Dict[int, ModelUpdate] = {}
        self.reputations: Dict[int, float] = {}
        self.merkle_roots: List[bytes] = []
        self.current_epoch = 0
        self.aggregation_ready = False
        self.lock = threading.Lock()

    def register_client(self, client_id: int, public_key: bytes):
        with self.lock:
            if client_id not in self.reputations:
                self.reputations[client_id] = self.config.initial_reputation
                print(f"‚úÖ Registered client {client_id} with reputation {self.config.initial_reputation}")

    def submit_update(self, client_id: int, merkle_root: bytes, zk_proof: bytes, signature: bytes, public_key: bytes) -> bool:
        with self.lock:
            if self.reputations.get(client_id, 0) < self.config.min_reputation:
                print(f"‚ùå Client {client_id} reputation too low: {self.reputations.get(client_id, 0)}")
                return False
            if self.config.use_zksnarks:
                if not self._verify_zk_proof(zk_proof, merkle_root):
                    self._slash_reputation(client_id)
                    print(f"‚ùå Invalid ZK proof for client {client_id}")
                    return False
            if not self._verify_signature(client_id, merkle_root + zk_proof, signature, public_key):
                self._slash_reputation(client_id)
                print(f"‚ùå Invalid signature for client {client_id}")
                return False
            update = ModelUpdate(client_id=client_id, merkle_root=merkle_root, zk_proof=zk_proof, signature=signature, timestamp=time.time(), reputation=self.reputations[client_id])
            self.updates[client_id] = update
            self.merkle_roots.append(merkle_root)
            print(f"‚úÖ Accepted update from client {client_id} (reputation: {self.reputations[client_id]:.2f})")
            return True

    def _verify_zk_proof(self, zk_proof: bytes, merkle_root: bytes) -> bool:
        return len(zk_proof) > 0

    def _verify_signature(self, client_id: int, data: bytes, signature: bytes, public_key: bytes) -> bool:
        return hashlib.sha256(signature).hexdigest()[:8] != "deadbeef"

    def _slash_reputation(self, client_id: int):
        current = self.reputations.get(client_id, self.config.initial_reputation)
        new_reputation = max(self.config.min_reputation, current * 0.9)
        self.reputations[client_id] = new_reputation
        print(f"üî® Slashed client {client_id} reputation: {current:.2f} ‚Üí {new_reputation:.2f}")

    def compute_aggregate_merkle_root(self) -> bytes:
        if not self.merkle_roots:
            return b""
        sorted_roots = sorted(self.merkle_roots)
        combined = b"".join(sorted_roots)
        return hashlib.sha256(combined).digest()

    def finalize_aggregation(self, ipfs_hash: str, merkle_proof: bytes, aggregator_id: int) -> bool:
        with self.lock:
            if not self.updates:
                return False
            agg_root = self.compute_aggregate_merkle_root()
            if not self._verify_merkle_proof(ipfs_hash, agg_root, merkle_proof):
                print(f"‚ùå Invalid Merkle proof from aggregator {aggregator_id}")
                return False
            print(f"‚úÖ Aggregation finalized for epoch {self.current_epoch} with {len(self.updates)} updates")
            self.aggregation_ready = True
            return True

    def _verify_merkle_proof(self, ipfs_hash: str, agg_root: bytes, proof: bytes) -> bool:
        return len(proof) > 0

    def clear_epoch(self):
        with self.lock:
            self.updates.clear()
            self.merkle_roots.clear()
            self.aggregation_ready = False
            self.current_epoch += 1

    def update_reputation(self, client_id: int, quality_score: float):
        with self.lock:
            current = self.reputations.get(client_id, self.config.initial_reputation)
            if quality_score > 0.7:
                new_reputation = min(self.config.max_reputation, current + self.config.reputation_reward)
                print(f"‚≠ê Increased reputation for client {client_id}: {current:.2f} ‚Üí {new_reputation:.2f}")
            elif quality_score < 0.3:
                new_reputation = max(self.config.min_reputation, current - 0.05)
                print(f"üîª Decreased reputation for client {client_id}: {current:.2f} ‚Üí {new_reputation:.2f}")
            else:
                new_reputation = current
            self.reputations[client_id] = new_reputation

# ==================== CORE: ATL CONTRACT ====================
@dataclass
class AuditRecord:
    epoch: int
    ipfs_hash: str
    merkle_root: bytes
    client_ids: List[int]
    timestamp: float
    tx_hash: str

class ATLContract:
    def __init__(self, config):
        self.config = config
        self.audit_log: List[AuditRecord] = []
        self.committed_checkpoints: Dict[int, bytes] = {}
        self.transaction_counter = 0

    def commit_aggregation(self, epoch: int, ipfs_hash: str, merkle_root: bytes, client_ids: List[int]) -> str:
        self.transaction_counter += 1
        tx_hash = f"0x{hashlib.sha256(f'{epoch}{ipfs_hash}'.encode()).hexdigest()}"
        record = AuditRecord(epoch=epoch, ipfs_hash=ipfs_hash, merkle_root=merkle_root, client_ids=client_ids, timestamp=time.time(), tx_hash=tx_hash)
        self.audit_log.append(record)
        self.committed_checkpoints[epoch] = merkle_root
        print(f"üîó ATL Commit: Epoch {epoch}, TX: {tx_hash[:16]}..., {len(client_ids)} clients")
        return tx_hash

# ==================== CORE: REPUTATION ====================
class ReputationManager:
    def __init__(self, config):
        self.config = config
        self.contribution_history: Dict[int, List[float]] = {}
        self.shapley_cache: Dict[int, float] = {}

    def calculate_shapley_values(self, client_updates: Dict[int, Dict], validation_fn, global_model) -> Dict[int, float]:
        client_ids = list(client_updates.keys())
        n_clients = len(client_ids)
        if n_clients == 0:
            return {}
        shapley_values = {cid: 0.0 for cid in client_ids}
        for _ in range(self.config.shapley_samples):
            permutation = random.sample(client_ids, n_clients)
            marginal_values = {}
            for i, client_id in enumerate(permutation):
                coalition_without = permutation[:i]
                coalition_with = permutation[:i+1]
                if len(coalition_without) == 0:
                    utility_without = validation_fn(global_model)
                else:
                    model_without = self._simulate_aggregation({cid: client_updates[cid] for cid in coalition_without})
                    utility_without = validation_fn(model_without)
                model_with = self._simulate_aggregation({cid: client_updates[cid] for cid in coalition_with})
                utility_with = validation_fn(model_with)
                marginal_values[client_id] = utility_with - utility_without
            for client_id in client_ids:
                shapley_values[client_id] += marginal_values.get(client_id, 0)
        for client_id in client_ids:
            shapley_values[client_id] /= self.config.shapley_samples
        self.shapley_cache.update(shapley_values)
        return shapley_values

    def _simulate_aggregation(self, updates: Dict[int, Dict]) -> Dict:
        if not updates:
            return {}
        aggregated = {}
        for update in updates.values():
            for key, value in update.items():
                if key not in aggregated:
                    aggregated[key] = torch.zeros_like(value)
                aggregated[key] += value
        for key in aggregated:
            aggregated[key] /= len(updates)
        return aggregated

    def calculate_rewards(self, shapley_values: Dict[int, float], reputations: Dict[int, float]) -> Dict[int, float]:
        rewards = {}
        total_weight = 0
        weighted_contributions = {}
        for client_id in shapley_values:
            weight = shapley_values[client_id] * reputations.get(client_id, self.config.initial_reputation)
            weighted_contributions[client_id] = max(weight, 0)
            total_weight += weighted_contributions[client_id]
        for client_id in shapley_values:
            if total_weight > 0:
                reward = (weighted_contributions[client_id] / total_weight) * self.config.total_reward_per_round
            else:
                reward = self.config.total_reward_per_round / len(shapley_values)
            reward = max(reward, self.config.participation_cost)
            rewards[client_id] = reward
        return rewards

# ==================== SIMULATOR ====================
class BlockFlowSimulator:
    def __init__(self, config: BlockFlowConfig):
        self.config = config
        self.clients: Dict[int, FLClient] = {}
        self.aggregators: Dict[int, Aggregator] = {}
        self.mcl = MCLContract(config)
        self.atl = ATLContract(config)
        self.reputation_mgr = ReputationManager(config)
        self.shard_manager = ShardManager(config.num_shards, config)
        self.global_model = self._create_model()
        self.global_weights = self.global_model.state_dict()
        self.metrics = {
            'accuracy': [],
            'participation_rate': [],
            'reputation_scores': [],
            'rewards': [],
            'throughput': [],
            'latency': []
        }

    def _create_model(self) -> nn.Module:
        if self.config.model_architecture == "cnn":
            return nn.Sequential(
                nn.Conv2d(1, 32, 3, 1), nn.ReLU(),
                nn.Conv2d(32, 64, 3, 1), nn.ReLU(),
                nn.MaxPool2d(2), nn.Flatten(),
                nn.Linear(9216, 128), nn.ReLU(),
                nn.Linear(128, 10)
            )
        else:
            return nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10))

    def setup_clients(self, num_clients: int, data_loaders):
        for i in range(num_clients):
            client = FLClient(
                client_id=i,
                data_loader=data_loaders[i] if i < len(data_loaders) else None,
                model=self._create_model(),
                config=self.config
            )
            self.clients[i] = client
            self.mcl.register_client(i, client.public_key)
            geo = f"region_{i % 5}"
            self.shard_manager.assign_client_to_shard(i, geo, client.reputation)
        print(f"‚úÖ Setup {len(self.clients)} clients across {self.config.num_shards} shards")

    def setup_aggregators(self):
        for i in range(self.config.num_shards):
            aggregator = Aggregator(i, self.config)
            self.aggregators[i] = aggregator
            self.shard_manager.shard_aggregators[i] = [i]
        print(f"‚úÖ Setup {len(self.aggregators)} aggregators")

    def run_training_round(self, round_num: int) -> Dict[str, Any]:
        print(f"\n‚ñ∂Ô∏è  Starting training round {round_num}")
        start_time = time.time()
        client_updates = {}

        for client_id, client in tqdm(self.clients.items(), desc="Client updates"):
            if random.random() > client.reputation:
                continue

            updates = client.local_training(self.global_weights)
            merkle_root, zk_proof, signature = client.generate_commitment(updates)

            if self.mcl.submit_update(client_id, merkle_root, zk_proof, signature, client.public_key):
                client_updates[client_id] = updates

        participation_rate = len(client_updates) / len(self.clients)
        print(f"üìä Participation rate: {participation_rate:.2%} ({len(client_updates)}/{len(self.clients)})")

        # Shard aggregation
        shard_updates = {}
        shard_reputations = {}
        shard_sizes = {}

        for shard_id in range(self.config.num_shards):
            shard_clients = self.shard_manager.get_shard_members(shard_id)
            shard_client_updates = {cid: client_updates[cid] for cid in shard_clients if cid in client_updates}

            if not shard_client_updates:
                continue

            aggregator = self.aggregators[shard_id]
            reputations = [self.clients[cid].reputation for cid in shard_client_updates]
            sizes = [1 for _ in shard_client_updates]  # Simplified

            aggregated_update, ipfs_hash = aggregator.aggregate_models(
                list(shard_client_updates.values()),
                reputations,
                sizes
            )

            shard_updates[shard_id] = aggregated_update
            shard_reputations[shard_id] = np.mean(reputations)
            shard_sizes[shard_id] = sum(sizes)

        # Global aggregation
        if shard_updates:
            global_aggregator = Aggregator(-1, self.config)
            final_update, final_ipfs_hash = global_aggregator.aggregate_models(
                list(shard_updates.values()),
                list(shard_reputations.values()),
                list(shard_sizes.values())
            )

            for name, param in final_update.items():
                self.global_weights[name] += param

            agg_root = self.mcl.compute_aggregate_merkle_root()

            if self.mcl.finalize_aggregation(final_ipfs_hash, b"merkle_proof_sim", -1):
                tx_hash = self.atl.commit_aggregation(
                    self.mcl.current_epoch,
                    final_ipfs_hash,
                    agg_root,
                    list(client_updates.keys())
                )
                self._distribute_rewards(client_updates)
                self._update_reputations(client_updates)

        self.mcl.clear_epoch()
        round_time = time.time() - start_time
        throughput = len(client_updates) / round_time if round_time > 0 else 0

        self.metrics['participation_rate'].append(participation_rate)
        self.metrics['throughput'].append(throughput)
        self.metrics['latency'].append(round_time)

        return {
            'round': round_num,
            'participation_rate': participation_rate,
            'throughput': throughput,
            'latency': round_time,
            'active_clients': len(client_updates)
        }

    def _distribute_rewards(self, client_updates: Dict[int, Dict]):
        def validation_fn(model):
            return np.random.beta(2, 2)

        shapley_values = self.reputation_mgr.calculate_shapley_values(
            client_updates,
            validation_fn,
            self.global_model
        )
        reputations = {cid: self.clients[cid].reputation for cid in client_updates}
        rewards = self.reputation_mgr.calculate_rewards(shapley_values, reputations)

        for client_id, reward in rewards.items():
            self.metrics['rewards'].append(reward)
            print(f"üí∞ Client {client_id} reward: {reward:.2f} tokens")

    def _update_reputations(self, client_updates: Dict[int, Dict]):
        for client_id in client_updates:
            quality_score = np.random.beta(2, 2)
            self.mcl.update_reputation(client_id, quality_score)
            self.clients[client_id].reputation = self.mcl.reputations[client_id]

    def run_simulation(self, num_rounds: int):
        print(f"üöÄ Starting BlockFlow simulation for {num_rounds} rounds")
        print(f"   Clients: {self.config.num_clients}")
        print(f"   Shards: {self.config.num_shards}")
        print(f"   ZK-SNARKs: {self.config.use_zksnarks}")

        results = []
        for round_num in range(num_rounds):
            result = self.run_training_round(round_num)
            results.append(result)

            if round_num > 0 and round_num % 10 == 0:
                self.shard_manager.rebalance_shards(
                    {cid: c.reputation for cid, c in self.clients.items()}
                )

        self._plot_metrics()
        return results

    def _plot_metrics(self):
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))

        axes[0, 0].plot(self.metrics['participation_rate'])
        axes[0, 0].set_title("Client Participation Rate")
        axes[0, 0].set_xlabel("Round")
        axes[0, 0].set_ylabel("Participation Rate")

        axes[0, 1].plot(self.metrics['throughput'])
        axes[0, 1].set_title("System Throughput")
        axes[0, 1].set_xlabel("Round")
        axes[0, 1].set_ylabel("Updates/sec")

        axes[1, 0].plot(self.metrics['latency'])
        axes[1, 0].set_title("Round Latency")
        axes[1, 0].set_xlabel("Round")
        axes[1, 0].set_ylabel("Latency (s)")

        if self.clients:
            reputations = [c.reputation for c in self.clients.values()]
            axes[1, 1].hist(reputations, bins=20)
            axes[1, 1].set_title("Reputation Distribution")
            axes[1, 1].set_xlabel("Reputation Score")
            axes[1, 1].set_ylabel("Number of Clients")

        plt.tight_layout()
        plt.savefig("/content/blockflow_metrics.png")
        print("üìä Metrics saved to /content/blockflow_metrics.png")
        plt.show()

# ==================== MAIN ====================
def main():
    print("üöÄ BlockFlow Simulator initializing...")

    config = BlockFlowConfig(
        num_clients=50,  # Reduced for Colab
        num_shards=5,
        use_zksnarks=True,
        total_reward_per_round=500.0
    )

    simulator = BlockFlowSimulator(config)

    # Setup mock data loaders
    from torch.utils.data import DataLoader, TensorDataset

    X = torch.randn(500, 1, 28, 28)  # Reduced dataset
    y = torch.randint(0, 10, (500,))
    dataset = TensorDataset(X, y)

    data_loaders = []
    for i in range(config.num_clients):
        start_idx = i * (500 // config.num_clients)
        end_idx = (i + 1) * (500 // config.num_clients)
        client_data = torch.utils.data.Subset(dataset, range(start_idx, end_idx))
        data_loaders.append(DataLoader(client_data, batch_size=config.batch_size))

    simulator.setup_clients(config.num_clients, data_loaders)
    simulator.setup_aggregators()

    results = simulator.run_simulation(num_rounds=10)  # Reduced rounds for speed

    print("\n" + "="*50)
    print("‚úÖ SIMULATION COMPLETE")
    print("="*50)
    print(f"Final participation rate: {results[-1]['participation_rate']:.2%}")
    print(f"Average throughput: {np.mean([r['throughput'] for r in results]):.2f} updates/sec")
    print(f"Average latency: {np.mean([r['latency'] for r in results]):.2f} seconds")

if __name__ == "__main__":
    main()